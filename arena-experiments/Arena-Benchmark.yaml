Arena-Benchmark:
    env:
        grid_search:
            # - Arena-Blowblow-Sparse-2T2P-Discrete
            # - Arena-BarrierGunner-3X3-PT-Sparse-2T1P-Discrete
            - PongNoFrameskip-v4
            # - Arena-Tennis-Sparse-2T1P-Discrete
    run: PPO
    local_dir: ~/Arena-Benchmark
    checkpoint_freq: 200
    checkpoint_at_end: True
    stop:
        training_iteration: 20000
    config:
        # === Environment Settings ===
        env_config:
            is_shuffle_agents:
                grid_search:
                    - True
                    # - False
            train_mode: True
            sensors:
                grid_search:
                    - [vector]
                    # - [visual_FP]
                    # - [visual_TP]
                    # - [visual_FP, visual_TP]
                    # - [vector, visual_FP]
            multi_agent_obs:
                grid_search:
                    - [own]
                    # - [team_relative]
                    # - [team_absolute]
                    # - [all_relative]
                    # - [all_absolute]
                    # - [own, team_relative]
        # === Multi-agent Settings ===
        iterations_per_reload:
            grid_search:
                - 10
        num_learning_policies:
            grid_search:
                - 1
                # - 2
                # - all
        playing_policy_load_recent_prob:
            grid_search:
                - 0.8
                # - 0.5
        size_population:
            grid_search:
                - 1
                # - 8
                # - 16
        share_layer_policies:
            grid_search:
                - []
                # - team
                # - [[0,2],[1,3]]
        actor_critic_obs:
            grid_search:
                - []
                # - [own, team_relative]
        # === Settings for Rollout Worker processes ===
        clip_rewards: None
        num_workers: 10
        num_envs_per_worker: 2
        sample_batch_size: 100
        batch_mode: truncate_episodes
        observation_filter: NoFilter
        # === Settings for the Trainer process ===
        num_gpus: 1
        train_batch_size: 5000
        # === PPO Settings ===
        lambda: 0.95
        kl_coeff: 0.5
        clip_param: 0.1
        vf_clip_param: 10.0
        entropy_coeff: 0.01
        sgd_minibatch_size: 500
        num_sgd_iter: 10
        vf_share_layers: True
        # === Debug Settings ===
